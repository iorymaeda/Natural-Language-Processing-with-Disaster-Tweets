# [Natural Language Processing with Disaster Tweets](https://www.kaggle.com/competitions/nlp-getting-started/overview)

There i'm trying to solve kaggle NLP task in a few ways:

1. Use BERT embeddings
2. Tune BERT
3. Train transformer text classifier from scratch


# References

Joeri R. Hermans, Gerasimos Spanakis, Rico MÃ¶ckel. Accumulated Gradient Normalization. [arxiv abs](https://arxiv.org/abs/1710.02368), 2017.

Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. [arxiv abs](https://arxiv.org/abs/1810.04805), 2019.

Jianwei Feng, Dong Huang. Optimal Gradient Checkpoint Search for Arbitrary Computation Graphs. [arxiv abs](https://arxiv.org/abs/1808.00079), 2021.
