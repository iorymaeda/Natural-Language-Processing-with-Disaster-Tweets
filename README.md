# [Natural Language Processing with Disaster Tweets](https://www.kaggle.com/competitions/nlp-getting-started/overview)

There i'm trying to solve kaggle NLP task in a few ways:

1. Use BERT embeddings
2. Tune BERT
3. Train transformer text classifier from scratch

# Results 

1. Gradient Accumulated + Gradient Checkpoint increase batch_size from ~16 to ~512


# References

Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. [arxiv abs](https://arxiv.org/abs/1810.04805), 2019.
