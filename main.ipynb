{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d05ae81-3e86-47eb-8580-58aae9c452df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\royta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch\n",
    "import torchmetrics\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d94f820-629a-47f2-916a-6018191946e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "def tokenize_function(examples, max_length):\n",
    "    return tokenizer(examples, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74b1691f-1066-45f7-aa88-db6807708cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─BertModel: 1-1                         --\n",
      "|    └─BertEmbeddings: 2-1               --\n",
      "|    |    └─Embedding: 3-1               (22,268,928)\n",
      "|    |    └─Embedding: 3-2               (393,216)\n",
      "|    |    └─Embedding: 3-3               (1,536)\n",
      "|    |    └─LayerNorm: 3-4               (1,536)\n",
      "|    |    └─Dropout: 3-5                 --\n",
      "|    └─BertEncoder: 2-2                  --\n",
      "|    |    └─ModuleList: 3-6              (85,054,464)\n",
      "|    └─BertPooler: 2-3                   --\n",
      "|    |    └─Linear: 3-7                  (590,592)\n",
      "|    |    └─Tanh: 3-8                    --\n",
      "├─Dropout: 1-2                           --\n",
      "├─Linear: 1-3                            1,538\n",
      "=================================================================\n",
      "Total params: 108,311,810\n",
      "Trainable params: 1,538\n",
      "Non-trainable params: 108,310,272\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "import torchsummary\n",
    "\n",
    "torchsummary.summary(model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bc4bdff-a7c0-4683-ae61-12cd080d43d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>48</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>49</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Est. September 2012 - Bristol</td>\n",
       "      <td>We always try to bring the heavy. #metal #RT h...</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>50</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>#AFRICANBAZE: Breaking news:Nigeria flag set a...</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>52</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>Crying out for more! Set me ablaze</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>53</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>London, UK</td>\n",
       "      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS...</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7575</th>\n",
       "      <td>10826</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>TN</td>\n",
       "      <td>On the bright side I wrecked http://t.co/uEa0t...</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7577</th>\n",
       "      <td>10829</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>#NewcastleuponTyne #UK</td>\n",
       "      <td>@widda16 ... He's gone. You can relax. I thoug...</td>\n",
       "      <td>0</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7579</th>\n",
       "      <td>10831</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Vancouver, Canada</td>\n",
       "      <td>Three days off from work and they've pretty mu...</td>\n",
       "      <td>0</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7580</th>\n",
       "      <td>10832</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>London</td>\n",
       "      <td>#FX #forex #trading Cramer: Iger's 3 words tha...</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7581</th>\n",
       "      <td>10833</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Lincoln</td>\n",
       "      <td>@engineshed Great atmosphere at the British Li...</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5080 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  keyword                       location  \\\n",
       "31       48   ablaze                     Birmingham   \n",
       "32       49   ablaze  Est. September 2012 - Bristol   \n",
       "33       50   ablaze                         AFRICA   \n",
       "34       52   ablaze               Philadelphia, PA   \n",
       "35       53   ablaze                     London, UK   \n",
       "...     ...      ...                            ...   \n",
       "7575  10826  wrecked                             TN   \n",
       "7577  10829  wrecked         #NewcastleuponTyne #UK   \n",
       "7579  10831  wrecked              Vancouver, Canada   \n",
       "7580  10832  wrecked                        London    \n",
       "7581  10833  wrecked                        Lincoln   \n",
       "\n",
       "                                                   text  target  len  \n",
       "31    @bbcmtd Wholesale Markets ablaze http://t.co/l...       1   55  \n",
       "32    We always try to bring the heavy. #metal #RT h...       0   67  \n",
       "33    #AFRICANBAZE: Breaking news:Nigeria flag set a...       1   82  \n",
       "34                   Crying out for more! Set me ablaze       0   34  \n",
       "35    On plus side LOOK AT THE SKY LAST NIGHT IT WAS...       0   76  \n",
       "...                                                 ...     ...  ...  \n",
       "7575  On the bright side I wrecked http://t.co/uEa0t...       0   51  \n",
       "7577  @widda16 ... He's gone. You can relax. I thoug...       0  107  \n",
       "7579  Three days off from work and they've pretty mu...       0  107  \n",
       "7580  #FX #forex #trading Cramer: Iger's 3 words tha...       0   93  \n",
       "7581  @engineshed Great atmosphere at the British Li...       0  104  \n",
       "\n",
       "[5080 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "df['len'] = df['text'].apply(lambda x: len(x))\n",
    "df[df['location'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697bf509-70f1-4602-926c-5b3e25dc7c14",
   "metadata": {},
   "source": [
    "# Classifier on BERT's embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844f515e-d8f4-4cd8-b252-0ab15858051b",
   "metadata": {},
   "source": [
    "### work with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "06de0db8-35ce-4add-9511-422465d462cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'd': [64, 32],\n",
    "    'dropout': 0.5,\n",
    "    'n_components': 384,\n",
    "    'batch_size': 1024,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b88537c2-8980-4a7b-9989-0973968b944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokenize_function(df['text'].tolist(), df['len'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4d2540d-4487-465f-aedf-768ecdf1a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "batch_size = 512\n",
    "\n",
    "num_batches = len(df)//batch_size\n",
    "if len(df)%batch_size != 0:\n",
    "    num_batches+= 1\n",
    "    \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in range(num_batches):\n",
    "        inputs = {\n",
    "            k: v[batch*batch_size:(batch+1)*batch_size].to(device) \n",
    "                  for k, v in x.items()\n",
    "        }\n",
    "        \n",
    "        outputs = model.bert(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            token_type_ids=inputs['token_type_ids'],\n",
    "        )\n",
    "        \n",
    "        X.append(outputs.pooler_output.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c296cbfe-13d9-4def-9082-1446dea5d3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "X = torch.cat(X)\n",
    "Y = df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "757246d8-f777-4b67-be8a-9f383a1aa977",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xval, Ytrain, Yval = train_test_split(X, Y, test_size=0.1, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ecc4ecc-206c-41cc-9fc9-9f58c68dfc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = utils.EmbeddingDataset(Xtrain, Ytrain)\n",
    "val_dataset = utils.EmbeddingDataset(Xval, Yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "edb803e3-aed6-46b2-8b40-e314ad4af74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(name):    \n",
    "    print(name)\n",
    "\n",
    "    checkpoint = torch.load(f'Models/W/{name}.torch')\n",
    "    classification_model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "    val_pred, val_true = trainer.val(val_dataloader)\n",
    "    val_pred = val_pred.softmax(1)\n",
    "    print('AUC:', auc(val_pred, val_true))\n",
    "    print('ACC:', acc(val_pred, val_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a2088bf2-3f6c-48fa-b9c6-e2591794129a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\royta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchmetrics\\utilities\\prints.py:36: UserWarning: Metric `AUROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "C:\\Users\\royta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "n_components = config['n_components']\n",
    "if n_components is not None:\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(Xtrain)\n",
    "    \n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=config['batch_size'])\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=config['batch_size'])\n",
    "\n",
    "classification_model = utils.ClassifierModel(\n",
    "    input_dim=n_components if n_components is not None else train_dataset.shape()[1], \n",
    "    callable_=None if n_components is None else pca.transform,\n",
    "    output_dim=2,\n",
    "    **config,\n",
    ").to(device)\n",
    "\n",
    "trainer = utils.Trainer(\n",
    "    model=classification_model,\n",
    "    metric=torchmetrics.AUROC(num_classes=2),\n",
    "    loss_fn=nn.CrossEntropyLoss(reduce=True),\n",
    "    optimizer=torch.optim.AdamW(classification_model.parameters(), lr=3e-4),\n",
    ")\n",
    "\n",
    "acc = torchmetrics.Accuracy(num_classes=2)\n",
    "auc = torchmetrics.AUROC(num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d08c0560-be6b-4176-abf4-899cdd7181c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'BERT embeddings + {0} + {1} + {2} + {3} '.format(\n",
    "    f\"{config['n_components']} pca\", \n",
    "    f\"{config['d']} d\",\n",
    "    f\"{config['dropout']} dropout\",\n",
    "    f\"{config['batch_size']} batchsize\",\n",
    ")\n",
    "board_name = name + datetime.datetime.now().strftime(\"%Y.%m.%d - %H-%M-%S\")\n",
    "\n",
    "log_dir = f\"logs/fit/{board_name}\"\n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "04ebe4b1-e35c-48b7-8bf3-ab643c11b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    wait = 0\n",
    "    patience = 150\n",
    "\n",
    "    epoch = 0\n",
    "    best_loss = -np.inf\n",
    "    while wait < patience:\n",
    "        train_loss = trainer.train(train_dataloader, epoch)\n",
    "\n",
    "        val_pred, val_true = trainer.val(val_dataloader)\n",
    "        val_pred = val_pred.softmax(1)\n",
    "        metrics = {\n",
    "            'AUC': auc(val_pred, val_true),\n",
    "            'ACC': acc(val_pred, val_true),\n",
    "        }\n",
    "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "        writer.add_scalar('AUC/train', trainer.metric.compute(), epoch)\n",
    "        writer.add_scalar('AUC/val', metrics['AUC'], epoch)\n",
    "        writer.add_scalar('ACC/val', metrics['ACC'], epoch)\n",
    "\n",
    "\n",
    "        wait+=1\n",
    "        epoch+=1\n",
    "        if metrics['AUC'] > best_loss:\n",
    "            checkpoint = trainer.checkpoint()\n",
    "            torch.save(checkpoint, f'Models/W/{name}.torch')\n",
    "            best_loss = metrics['AUC']\n",
    "            wait = 0\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"KeyboardInterrupt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "10f4795f-a73a-4421-80bb-db05a8d38d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT embeddings + 384 pca + [64, 32] d + 0.5 dropout + 1024 batchsize \n",
      "AUC: tensor(0.8214)\n",
      "ACC: tensor(0.7717)\n"
     ]
    }
   ],
   "source": [
    "validate(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd4189e3-5c53-4304-9d1e-6eedff3786f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT embeddings + 512 pca + [64, 32] d + 0.15 dropout + 1024 batchsize \n",
      "AUC: tensor(0.8257)\n",
      "ACC: tensor(0.7730)\n"
     ]
    }
   ],
   "source": [
    "validate(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b36e46f8-b1c0-4f9b-bf70-2cdef1fc287b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT embeddings + 256 pca + [] d + 0.15 dropout \n",
      "AUC: tensor(0.8196)\n",
      "ACC: tensor(0.7743)\n"
     ]
    }
   ],
   "source": [
    "# 2048 batch\n",
    "validate(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a16f1799-9d93-4d51-9207-d4f19cbab42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT embeddings + 256 pca + [64, 32] d + 0.15 dropout \n",
      "AUC: tensor(0.8231)\n",
      "ACC: tensor(0.7808)\n"
     ]
    }
   ],
   "source": [
    "# 2048 batch\n",
    "validate(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf2dad33-8531-4914-8df8-461e796cdfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT embeddings + 256 pca + [128, 128, 64] d + 0.15 dropout \n",
      "AUC: tensor(0.8211)\n",
      "ACC: tensor(0.7835)\n"
     ]
    }
   ],
   "source": [
    "# 2048 batch\n",
    "validate(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0a69eda8-0829-4331-924f-75df99fe5536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT embeddings + 256 pca + [128, 64] d + 0.15 dropout \n",
      "AUC: tensor(0.8247)\n",
      "ACC: tensor(0.7782)\n"
     ]
    }
   ],
   "source": [
    "# 2048 batch\n",
    "validate(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "36a1d5f1-c216-4713-beab-d6c9c2e6b486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT embeddings + 256 pca + [128, 64] d \n",
      "AUC: tensor(0.8223)\n",
      "ACC: tensor(0.7769)\n"
     ]
    }
   ],
   "source": [
    "# 2048 batch\n",
    "# 0.5 dropout\n",
    "validate(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "73263e79-cede-484a-af6f-8319737841da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT embeddings + 512 pca + [128, 64] d \n",
      "AUC: tensor(0.8231)\n",
      "ACC: tensor(0.7769)\n"
     ]
    }
   ],
   "source": [
    "# 2048 batch\n",
    "# 0.5 dropout\n",
    "validate(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f91ad013-2077-407b-8676-473ada045e4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT embeddings + None pca + [128, 64] d\n",
      "AUC: tensor(0.8180)\n",
      "ACC: tensor(0.7677)\n"
     ]
    }
   ],
   "source": [
    "# 2048 batch\n",
    "# 0.5 dropout\n",
    "validate(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bba5c07-edcc-4f99-8b15-07a5789f76eb",
   "metadata": {},
   "source": [
    "# Tune BERT\n",
    "\n",
    "w.i.p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a68cfdf4-9ff9-423b-a54e-43e5599fe9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, batch_size, **kwargs):\n",
    "        self.df = df.reset_index()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.values = df[['text', 'target']].values\n",
    "        \n",
    "        \n",
    "    def tokenize_function(self, examples):\n",
    "        return tokenizer(examples, padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx = random.choices(range(len(self)), k=self.batch_size)\n",
    "        x = self.tokenize_function(self.values[idx, 0].tolist())\n",
    "        y = self.values[idx, 1].astype('int32')\n",
    "        \n",
    "        x['labels'] = torch.LongTensor(y)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8a06a67-8634-47c7-a8ff-833fca455ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = transformers.get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e48433ca-05da-4699-8660-45a5939232d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                                                          | 120/19839 [06:35<18:25:30,  3.36s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m----> 6\u001b[0m         batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m      7\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(\n\u001b[0;32m      8\u001b[0m             labels\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      9\u001b[0m             input_ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     10\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     11\u001b[0m             token_type_ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     12\u001b[0m         )\n\u001b[0;32m     13\u001b[0m         loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m----> 6\u001b[0m         batch \u001b[38;5;241m=\u001b[39m {k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m      7\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(\n\u001b[0;32m      8\u001b[0m             labels\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      9\u001b[0m             input_ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     10\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     11\u001b[0m             token_type_ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     12\u001b[0m         )\n\u001b[0;32m     13\u001b[0m         loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(\n",
    "            labels=batch['labels'],\n",
    "            input_ids=batch['input_ids'][0],\n",
    "            attention_mask=batch['attention_mask'][0],\n",
    "            token_type_ids=batch['token_type_ids'][0],\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9852f0-7ec0-4e59-837c-a3d6b9959df1",
   "metadata": {},
   "source": [
    "# End to end transformer classifier\n",
    "\n",
    "w.i.p."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
