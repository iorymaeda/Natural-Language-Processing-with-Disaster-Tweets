{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d05ae81-3e86-47eb-8580-58aae9c452df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\royta\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch\n",
    "import torchmetrics\n",
    "import transformers\n",
    "import torch.nn as nn\n",
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d94f820-629a-47f2-916a-6018191946e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "def tokenize_function(examples, max_length):\n",
    "    return tokenizer(examples, max_length=max_length, padding=\"max_length\", truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aa2c19e-11e9-4d15-b4e7-2e55b1ab91fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74b1691f-1066-45f7-aa88-db6807708cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================\n",
      "Layer (type:depth-idx)                   Param #\n",
      "=================================================================\n",
      "├─BertModel: 1-1                         --\n",
      "|    └─BertEmbeddings: 2-1               --\n",
      "|    |    └─Embedding: 3-1               (22,268,928)\n",
      "|    |    └─Embedding: 3-2               (393,216)\n",
      "|    |    └─Embedding: 3-3               (1,536)\n",
      "|    |    └─LayerNorm: 3-4               (1,536)\n",
      "|    |    └─Dropout: 3-5                 --\n",
      "|    └─BertEncoder: 2-2                  --\n",
      "|    |    └─ModuleList: 3-6              (85,054,464)\n",
      "|    └─BertPooler: 2-3                   --\n",
      "|    |    └─Linear: 3-7                  (590,592)\n",
      "|    |    └─Tanh: 3-8                    --\n",
      "├─Dropout: 1-2                           --\n",
      "├─Linear: 1-3                            1,538\n",
      "=================================================================\n",
      "Total params: 108,311,810\n",
      "Trainable params: 1,538\n",
      "Non-trainable params: 108,310,272\n",
      "=================================================================\n"
     ]
    }
   ],
   "source": [
    "import torchsummary\n",
    "\n",
    "torchsummary.summary(model);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bc4bdff-a7c0-4683-ae61-12cd080d43d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>48</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Birmingham</td>\n",
       "      <td>@bbcmtd Wholesale Markets ablaze http://t.co/l...</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>49</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Est. September 2012 - Bristol</td>\n",
       "      <td>We always try to bring the heavy. #metal #RT h...</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>50</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>AFRICA</td>\n",
       "      <td>#AFRICANBAZE: Breaking news:Nigeria flag set a...</td>\n",
       "      <td>1</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>52</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>Philadelphia, PA</td>\n",
       "      <td>Crying out for more! Set me ablaze</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>53</td>\n",
       "      <td>ablaze</td>\n",
       "      <td>London, UK</td>\n",
       "      <td>On plus side LOOK AT THE SKY LAST NIGHT IT WAS...</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7575</th>\n",
       "      <td>10826</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>TN</td>\n",
       "      <td>On the bright side I wrecked http://t.co/uEa0t...</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7577</th>\n",
       "      <td>10829</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>#NewcastleuponTyne #UK</td>\n",
       "      <td>@widda16 ... He's gone. You can relax. I thoug...</td>\n",
       "      <td>0</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7579</th>\n",
       "      <td>10831</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Vancouver, Canada</td>\n",
       "      <td>Three days off from work and they've pretty mu...</td>\n",
       "      <td>0</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7580</th>\n",
       "      <td>10832</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>London</td>\n",
       "      <td>#FX #forex #trading Cramer: Iger's 3 words tha...</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7581</th>\n",
       "      <td>10833</td>\n",
       "      <td>wrecked</td>\n",
       "      <td>Lincoln</td>\n",
       "      <td>@engineshed Great atmosphere at the British Li...</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5080 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  keyword                       location  \\\n",
       "31       48   ablaze                     Birmingham   \n",
       "32       49   ablaze  Est. September 2012 - Bristol   \n",
       "33       50   ablaze                         AFRICA   \n",
       "34       52   ablaze               Philadelphia, PA   \n",
       "35       53   ablaze                     London, UK   \n",
       "...     ...      ...                            ...   \n",
       "7575  10826  wrecked                             TN   \n",
       "7577  10829  wrecked         #NewcastleuponTyne #UK   \n",
       "7579  10831  wrecked              Vancouver, Canada   \n",
       "7580  10832  wrecked                        London    \n",
       "7581  10833  wrecked                        Lincoln   \n",
       "\n",
       "                                                   text  target  len  \n",
       "31    @bbcmtd Wholesale Markets ablaze http://t.co/l...       1   55  \n",
       "32    We always try to bring the heavy. #metal #RT h...       0   67  \n",
       "33    #AFRICANBAZE: Breaking news:Nigeria flag set a...       1   82  \n",
       "34                   Crying out for more! Set me ablaze       0   34  \n",
       "35    On plus side LOOK AT THE SKY LAST NIGHT IT WAS...       0   76  \n",
       "...                                                 ...     ...  ...  \n",
       "7575  On the bright side I wrecked http://t.co/uEa0t...       0   51  \n",
       "7577  @widda16 ... He's gone. You can relax. I thoug...       0  107  \n",
       "7579  Three days off from work and they've pretty mu...       0  107  \n",
       "7580  #FX #forex #trading Cramer: Iger's 3 words tha...       0   93  \n",
       "7581  @engineshed Great atmosphere at the British Li...       0  104  \n",
       "\n",
       "[5080 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "df['len'] = df['text'].apply(lambda x: len(x))\n",
    "df[df['location'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697bf509-70f1-4602-926c-5b3e25dc7c14",
   "metadata": {},
   "source": [
    "# Classifier on BERT's embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844f515e-d8f4-4cd8-b252-0ab15858051b",
   "metadata": {},
   "source": [
    "### work with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06de0db8-35ce-4add-9511-422465d462cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'd': [128, 64],\n",
    "    'n_components': None,\n",
    "    'batch_size': 2048,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b88537c2-8980-4a7b-9989-0973968b944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokenize_function(df['text'].tolist(), df['len'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4d2540d-4487-465f-aedf-768ecdf1a3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "batch_size = 512\n",
    "\n",
    "num_batches = len(df)//batch_size\n",
    "if len(df)%batch_size != 0:\n",
    "    num_batches+= 1\n",
    "    \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in range(num_batches):\n",
    "        inputs = {\n",
    "            k: v[batch*batch_size:(batch+1)*batch_size].to(device) \n",
    "                  for k, v in x.items()\n",
    "        }\n",
    "        \n",
    "        outputs = model.bert(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            token_type_ids=inputs['token_type_ids'],\n",
    "        )\n",
    "        \n",
    "        X.append(outputs.pooler_output.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c296cbfe-13d9-4def-9082-1446dea5d3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "X = torch.cat(X)\n",
    "Y = df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26336b6d-4baa-4697-807e-065a966c2828",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, x, y, **kwargs):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def shape(self):\n",
    "        return self.x.shape\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.x[idx]\n",
    "        y = self.y[idx]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "757246d8-f777-4b67-be8a-9f383a1aa977",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain, Xval, Ytrain, Yval = train_test_split(X, Y, test_size=0.1, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ecc4ecc-206c-41cc-9fc9-9f58c68dfc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EmbeddingDataset(Xtrain, Ytrain)\n",
    "val_dataset = EmbeddingDataset(Xval, Yval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33e7f045-c0d2-4dec-8135-268e534510a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, \n",
    "        model, loss_fn, optimizer, \n",
    "        stop_batch=None, metric=None,\n",
    "        device='cuda', fp16=False, \n",
    "        **kwargs):\n",
    "        self.model: nn.Module = model\n",
    "        self.device = device\n",
    "        self.metric = metric\n",
    "        \n",
    "        self.stop_batch = 100**100 if stop_batch is None else stop_batch\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer  \n",
    "        \n",
    "        self.fp16 = fp16\n",
    "        if fp16:\n",
    "            self.scaler = torch.cuda.amp.GradScaler()        \n",
    "        \n",
    "        \n",
    "    def checkpoint(self) -> dict:\n",
    "        cpoint =  {\n",
    "            \"model\": self.model.state_dict(),\n",
    "            \"optimizer\": self.optimizer.state_dict(),\n",
    "        }\n",
    "        if self.fp16:\n",
    "            cpoint[\"scaler\"] = self.scaler.state_dict()\n",
    "            \n",
    "        return cpoint\n",
    "    \n",
    "        \n",
    "    def train(self, dataset, epoch) -> float:\n",
    "        self.model.train()\n",
    "        running_loss = 0\n",
    "        for idx, batch in enumerate(dataset):\n",
    "            X, Y = batch\n",
    "            X, Y = X.to(self.device), Y.to(self.device)\n",
    "            running_loss+= self.__train(X, Y)\n",
    "            if idx >= self.stop_batch:\n",
    "                break\n",
    "                \n",
    "        return running_loss\n",
    "            \n",
    "            \n",
    "    def val(self, dataset) -> list[torch.Tensor]:\n",
    "        self.model.eval()\n",
    "        val_pred, val_true = [], []\n",
    "        with torch.inference_mode():\n",
    "            for batch in dataset:\n",
    "                X, Y = batch\n",
    "                val_pred+= [self.model(X.to(self.device)).cpu()]\n",
    "                val_true+= [Y]\n",
    "        return torch.cat(val_pred), torch.cat(val_true)\n",
    "                \n",
    "            \n",
    "    def __train(self, X, Y) -> float:\n",
    "        self.optimizer.zero_grad()\n",
    "        if self.fp16:\n",
    "            with torch.cuda.amp.autocast(enabled=True):\n",
    "                outputs = self.model(X).softmax(1)\n",
    "                loss = self.loss_fn(outputs, Y)\n",
    "                \n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "\n",
    "        else:\n",
    "            outputs = self.model(X).softmax(1)\n",
    "            loss = self.loss_fn(outputs, Y)    \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        if self.metric:\n",
    "            self.metric(outputs, Y)\n",
    "            \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5444e25f-1f72-4bb9-a9b7-7ed5641f007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim=32, output_dim=2, d=[64, 64], callable_=None, **kwargs):\n",
    "        super().__init__()\n",
    "        \n",
    "        seq = []\n",
    "        d = [input_dim] + d + [output_dim]\n",
    "        for i in range(len(d)-1):\n",
    "            seq.append(\n",
    "                nn.Linear(d[i], d[i+1])\n",
    "            )\n",
    "            seq.append(nn.Dropout(p=0.5))\n",
    "            if i != len(d)-2:\n",
    "                seq.append(nn.GELU())\n",
    "                \n",
    "        self.callable_ = callable_\n",
    "        self.seq = nn.Sequential(*seq)\n",
    "\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if self.callable_ is not None:\n",
    "            device = x.device\n",
    "            x = torch.from_numpy(self.callable_(x.cpu()).astype('float32'))\n",
    "            x = x.to(device)\n",
    "            \n",
    "        return self.seq(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a2088bf2-3f6c-48fa-b9c6-e2591794129a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = config['n_components']\n",
    "if n_components is not None:\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(Xtrain)\n",
    "    \n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=config['batch_size'])\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=config['batch_size'])\n",
    "\n",
    "classification_model = Model(\n",
    "    input_dim=n_components if n_components is not None else train_dataset.shape()[1], \n",
    "    callable_=None if n_components is None else pca.transform,\n",
    "    output_dim=2,\n",
    "    d=config['d'],\n",
    ").to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=classification_model,\n",
    "    metric=torchmetrics.AUROC(num_classes=2),\n",
    "    loss_fn=nn.CrossEntropyLoss(reduce=True),\n",
    "    optimizer=torch.optim.AdamW(classification_model.parameters(), lr=3e-4),\n",
    ")\n",
    "\n",
    "acc = torchmetrics.Accuracy(num_classes=2)\n",
    "auc = torchmetrics.AUROC(num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d08c0560-be6b-4176-abf4-899cdd7181c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'BERT embeddings + {0} + {1} '.format(\n",
    "    f\"{config['n_components']} pca\", \n",
    "    f\"{config['d']} d\"\n",
    ")\n",
    "board_name = name + datetime.datetime.now().strftime(\"%Y.%m.%d - %H-%M-%S\")\n",
    "\n",
    "log_dir = f\"logs/fit/{board_name}\"\n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04ebe4b1-e35c-48b7-8bf3-ab643c11b430",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    wait = 0\n",
    "    patience = 100\n",
    "\n",
    "    epoch = 0\n",
    "    best_loss = -np.inf\n",
    "    while wait < patience:\n",
    "        train_loss = trainer.train(train_dataloader, epoch)\n",
    "\n",
    "        val_pred, val_true = trainer.val(val_dataloader)\n",
    "        val_pred = val_pred.softmax(1)\n",
    "        metrics = {\n",
    "            'AUC': auc(val_pred, val_true),\n",
    "            'ACC': acc(val_pred, val_true),\n",
    "        }\n",
    "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "        writer.add_scalar('AUC/train', trainer.metric.compute(), epoch)\n",
    "        writer.add_scalar('AUC/val', metrics['AUC'], epoch)\n",
    "        writer.add_scalar('ACC/val', metrics['ACC'], epoch)\n",
    "\n",
    "\n",
    "        wait+=1\n",
    "        epoch+=1\n",
    "        if metrics['AUC'] > best_loss:\n",
    "            checkpoint = trainer.checkpoint()\n",
    "            torch.save(checkpoint, f'Models/W/{name}.torch')\n",
    "            best_loss = metrics['AUC']\n",
    "            wait = 0\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"KeyboardInterrupt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12d71fcc-8e88-4b86-9134-1a4de1718c81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT embeddings + None pca + [128, 64] d\n",
      "AUC: tensor(0.8180)\n",
      "ACC: tensor(0.7677)\n"
     ]
    }
   ],
   "source": [
    "print(name)\n",
    "\n",
    "checkpoint = torch.load(f'Models/W/{name}.torch')\n",
    "classification_model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "val_pred, val_true = trainer.val(val_dataloader)\n",
    "val_pred = val_pred.softmax(1)\n",
    "print('AUC:', auc(val_pred, val_true))\n",
    "print('ACC:', acc(val_pred, val_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cd73398-de42-40b2-9afb-77cd2ea7ccf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT embeddings + None pca + [256, 256] d\n",
      "AUC: tensor(0.8111)\n",
      "ACC: tensor(0.7598)\n"
     ]
    }
   ],
   "source": [
    "print(name)\n",
    "\n",
    "checkpoint = torch.load(f'Models/W/{name}.torch')\n",
    "classification_model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "val_pred, val_true = trainer.val(val_dataloader)\n",
    "val_pred = val_pred.softmax(1)\n",
    "print('AUC:', auc(val_pred, val_true))\n",
    "print('ACC:', acc(val_pred, val_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "b8f36939-062d-49f2-a312-2bd3c8a4170c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT embeddings + pca + linears\n",
      "AUC: tensor(0.8246)\n",
      "ACC: tensor(0.7795)\n"
     ]
    }
   ],
   "source": [
    "print(name)\n",
    "\n",
    "checkpoint = torch.load(f'Models/W/{name}.torch')\n",
    "classification_model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "val_pred, val_true = trainer.val(val_dataloader)\n",
    "val_pred = val_pred.softmax(1)\n",
    "print('AUC:', auc(val_pred, val_true))\n",
    "print('ACC:', acc(val_pred, val_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef3e55c3-273e-4eb8-b49f-f632c3840bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: tensor(0.8505)\n",
      "ACC: tensor(0.7720)\n"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load(f'Models/W/{name}.torch')\n",
    "classification_model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "val_pred, val_true = trainer.val(val_dataloader)\n",
    "val_pred = val_pred.softmax(1)\n",
    "print('AUC:', auc(val_pred, val_true))\n",
    "print('ACC:', acc(val_pred, val_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bba5c07-edcc-4f99-8b15-07a5789f76eb",
   "metadata": {},
   "source": [
    "# Tune BERT\n",
    "\n",
    "w.i.p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a68cfdf4-9ff9-423b-a54e-43e5599fe9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, batch_size, **kwargs):\n",
    "        self.df = df.reset_index()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.values = df[['text', 'target']].values\n",
    "        \n",
    "        \n",
    "    def tokenize_function(self, examples):\n",
    "        return tokenizer(examples, padding=\"max_length\", truncation=True, return_tensors='pt')\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        idx = random.choices(range(len(self)), k=self.batch_size)\n",
    "        x = self.tokenize_function(self.values[idx, 0].tolist())\n",
    "        y = self.values[idx, 1].astype('int32')\n",
    "        \n",
    "        x['labels'] = torch.LongTensor(y)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8a06a67-8634-47c7-a8ff-833fca455ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = transformers.get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e48433ca-05da-4699-8660-45a5939232d9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                                                                          | 120/19839 [06:35<18:25:30,  3.36s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m----> 6\u001b[0m         batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m      7\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(\n\u001b[0;32m      8\u001b[0m             labels\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      9\u001b[0m             input_ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     10\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     11\u001b[0m             token_type_ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     12\u001b[0m         )\n\u001b[0;32m     13\u001b[0m         loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[1;32m----> 6\u001b[0m         batch \u001b[38;5;241m=\u001b[39m {k: \u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m      7\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m model(\n\u001b[0;32m      8\u001b[0m             labels\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m      9\u001b[0m             input_ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     10\u001b[0m             attention_mask\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     11\u001b[0m             token_type_ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m     12\u001b[0m         )\n\u001b[0;32m     13\u001b[0m         loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(\n",
    "            labels=batch['labels'],\n",
    "            input_ids=batch['input_ids'][0],\n",
    "            attention_mask=batch['attention_mask'][0],\n",
    "            token_type_ids=batch['token_type_ids'][0],\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9852f0-7ec0-4e59-837c-a3d6b9959df1",
   "metadata": {},
   "source": [
    "# End to end transformer classifier\n",
    "\n",
    "w.i.p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dc9c29-c45b-47fe-89d8-412941b56e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
